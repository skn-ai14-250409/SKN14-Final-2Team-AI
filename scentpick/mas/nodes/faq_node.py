from ..config import llm
from langchain_core.messages import  HumanMessage, AIMessage
from ..state import AgentState
from ..prompts.faq_prompt import faq_prompt


def FAQ_agent_node(state: AgentState) -> AgentState:
    """FAQ agent - LLM ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ í–¥ìˆ˜ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€"""
    user_query = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break
    if not user_query:
        user_query = "(empty)"
    
    try:
        # LLMì—ê²Œ í–¥ìˆ˜ ì§€ì‹ ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì„¤ì •

        
        chain = faq_prompt | llm
        result = chain.invoke({"question": user_query})
        
        # ê²°ê³¼ë¥¼ í¬ë§·íŒ…
        final_answer = f"ğŸ“š **í–¥ìˆ˜ ì§€ì‹**\n\n{result.content}"
        
        msgs = state["messages"] + [AIMessage(content=final_answer)]
        return {
            "messages": msgs, 
            "next": None, 
            "router_json": state.get("router_json")
        }
    except Exception as e:
        error_msg = f"âŒ í–¥ìˆ˜ ì§€ì‹ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        msgs = state["messages"] + [AIMessage(content=error_msg)]
        return {
            "messages": msgs, 
            "next": None, 
            "router_json": state.get("router_json")
        }
